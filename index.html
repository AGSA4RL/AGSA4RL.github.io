<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Policy Optimization under Imperfect Human Interactions with Agent-Gated Shared Autonomy</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <!-- <div class="logo">
      <a href="https://genforce.github.io/" target="_blank"><img src="./assets/genforce.png"></a>
    </div> -->
    <div class="title", style="padding-top: 25pt;">  <!-- Set padding as 10 if title is with two lines. -->
     Policy Optimization under Imperfect Human <br> Interactions with Agent-Gated Shared Autonomy
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <p style="text-align:center">Submitted to ICLR 2025</p>
    <a href="#" target="_blank">Anonymous Authors</a>&nbsp;&nbsp;

</div>

  <!-- <div class="teaser">
    <img src="https://via.placeholder.com/300x100">
  </div> -->
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Overview</div>
  <div class="teaser" style="width: 85%; margin-left: 70pt">
    <img src="assets/intro-1.png">
    <div class="text">
        <br>
        Fig. 1: Overview of the proposed method.
    </div>
</div>
  <div class="body">
    As shown in Fig. 1, The learning agent (in purple) interacts with the environment under the monitoring of the gating agent (in black). 
    The gating agent decides when to request human intervention.
    Learning agent trajectories are in green and human trajectories are in red and yellow. 
    Human feedbacks are denoted with thumbs up and down. 
    Feedbacks at \(t_1\) and \(t_3\) are human evaluations on whether the gating agent triggers control switch at proper timesteps. 
    Feedbacks at \(t_2\) and \(t_4\) are human preferences on whether the current intervention trajectory is better than the previous one. 
    For example, the trajectory between \(t_3\) and \(t_4\) is better than that between \(t_1\) and \(t_2\), so human may provide positive feedback on \(t_4\).
  </div>
</div>
<!-- === Overview Section Ends === -->

<div class="section">
  <div class="title">Method</div>
  <div class="body">

    </div>
  <div class="teaser" style="width: 85%; margin-left: 70pt">
    <img src="assets/method-1.png">
    <div class="text">
        <br>
        Fig. 2:   The framework for training both the gating agent and the learning agent. 
    </div>
  </div>
  <div class="body">
    To properly train the gating value function \(Q_g\) for optimal intervention timing, human participants follow three steps, as illustrated in Fig. 2: <br>
    <ol>
        <li> 
          Providing a binary signal \(I(s_t)\) that assesses whether the current environment state is indeed worth intervention and is intervened in time. 
        </li>
           <!-- Such human evaluation provides a direct feedback on whether gating agent's intervention decisions successfully indicates dangerous or unexplored areas. 
           Its advantage over directly imitating human intervention decisions is that humans have time to examine the intervention quality, 
           rather than making real-time decisions that can be influenced by tiredness, carelessness, or network latency.  -->
        <li> 
          Interacting with the environment for \(T\) steps and offering online demonstration segment 
           \(\sigma=(s_t,a^{\text{human}}_t,\dots,s_{t+T-1},a^{\text{human}}_{t+T-1})\), 
           aiming at guiding the learning agent out of the region that is dangerous or no longer needs exploration.
        </li>
        <li> 
          Providing a preference signal \(p_t=P_\psi[\sigma\succ \sigma']\in\{0, 0.5, 1\}\), 
           indicating whether current segment  \(\sigma\) is better than the previous segment \(\sigma'\). 
          </li>

      </ol>
           <!-- As human participants are familiar with recent trajectories of themselves, this way of preference pair construction 
           saves the burden for humans of reviewing previously sampled trajectories.
           Bad human samples can happen due to imperfect human behaviors or untimely intervention decision of the gating agent. 
           By assigning low human preference on these samples, the gating agent can learn from human demonstrations at all performance levels 
           and mistakes in the intervention decision made by itself. -->
   </div>
   <div class="body">

   </div>
</div> 
<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Results</div>
  <div class="body">

    <div class="teaser" style="width: 85%; margin-left: 50pt">
      <img src="assets/walker2d_ablation-1.png">
      <div class="text">
          <br>
          Fig. 3: Algorithm performance curves for the ablation study in the Walker2d environment. 
          The lines are average return across four different trials and the shadow areas denote the standard deviation.
      </div>
    </div>
    We present performance curves for ablation studies in the Walker2d environment in Fig. 3. We consider the following methods:
    
    <ul>
    <li>“AGSA w/ Failure Prediction” and “w/ Ensemble”
    are alternative approaches for constructing the gating agent. Both methods keep the learning agent
    training unchanged. </li>
    <li>“AGSA w/o Human Preference Feedback” and “w/o Human Evaluative Feedback”
    remove \(r_\psi (s_t, a_t)\) and \(I(s_t)\) respectively when computing the gating agent reward \(r_G
    \). </li>
    <li>
    “AGSA w/ \(r_\psi\) as \(r_\pi\) ” refers to using the reward model \(r_\psi\) trained from human preference feedback as the proxy
    reward \(r_\pi\) to train the learning agent, in the same way as PbRL algorithms.</li>
    </ul>


    In the following video, we visualize two stages of training in the Walker2d environment, both with and without simulated human intervention.
    At the early stage of training, the learning agent is likely to fall down without intervention due to its poor ability to balance.
    Instead, the gating agent can trigger simulated human intervention to prevent the learning agent from falling down, leading to a good behavior
    policy that facilitates the exploration process.
    Near the end of training, the learning agent can walk stably without intervention, and the gating agent can learn to avoid unnecessary intervention.

    <!-- Insert Video -->
    <div class="teaser" style="width: 85%; margin-left: 50pt">
      <video width="100%" controls>
        <source src="assets/mujoco_plot.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <div class="text">
          Video 1: Training visualization in the Walker2d environment both with and without simulated human intervention.
      </div>

  </div>
</div>
<!-- === Result Section Ends === -->


</body>
</html>
