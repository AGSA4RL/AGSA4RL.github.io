<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Policy Optimization under Imperfect Human Interactions with Agent-Gated Shared Autonomy</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <!-- <div class="logo">
      <a href="https://genforce.github.io/" target="_blank"><img src="./assets/genforce.png"></a>
    </div> -->
    <div class="title", style="padding-top: 25pt;">  <!-- Set padding as 10 if title is with two lines. -->
     Policy Optimization under Imperfect Human <br> Interactions with Agent-Gated Shared Autonomy
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <p style="text-align:center">Submitted to ICLR 2025</p>
    <a href="#" target="_blank">Anonymous Authors</a>&nbsp;&nbsp;

</div>

  <!-- <div class="teaser">
    <img src="https://via.placeholder.com/300x100">
  </div> -->
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Overview</div>
  <div class="teaser" style="width: 85%; margin-left: 70pt">
    <img src="assets/intro-1.png">
    <div class="text">
        <br>
        Fig. 1: Overview of the proposed method.
    </div>
</div>
  <div class="body">
    As shown in Fig. 1, The learning agent (in purple) interacts with the environment under the monitoring of the gating agent (in black). 
    The gating agent decides when to request human intervention.
    Learning agent trajectories are in green and human trajectories are in red and yellow. 
    Human feedbacks are denoted with thumbs up and down. 
    Feedbacks at \(t_1\) and \(t_3\) are human evaluations on whether the gating agent triggers control switch at proper timesteps. 
    Feedbacks at \(t_2\) and \(t_4\) are human preferences on whether the current intervention trajectory is better than the previous one. 
    For example, the trajectory between \(t_3\) and \(t_4\) is better than that between \(t_1\) and \(t_2\), so human may provide positive feedback on \(t_4\).
  </div>
</div>
<!-- === Overview Section Ends === -->

<div class="section">
  <div class="title">Method</div>
  <div class="body">

    </div>
  <div class="teaser" style="width: 85%; margin-left: 70pt">
    <img src="assets/method-1.png">
    <div class="text">
        <br>
        Fig. 2:   The framework for training both the gating agent and the learning agent. 
    </div>
  </div>
  <div class="body">
    To properly train the gating value function \(Q_g\) for optimal intervention timing, human participants follow three steps, as illustrated in Fig. 3 (upper): <br>
        &ensp; 1. Providing a binary signal \(I(s_t)\) that assesses whether the current environment state is indeed worth intervention and is intervened in time. 
           <!-- Such human evaluation provides a direct feedback on whether gating agent's intervention decisions successfully indicates dangerous or unexplored areas. 
           Its advantage over directly imitating human intervention decisions is that humans have time to examine the intervention quality, 
           rather than making real-time decisions that can be influenced by tiredness, carelessness, or network latency.  -->
           <br>
        &ensp; 2. Interacting with the environment for \(T\) steps and offering online demonstration segment 
           \(\sigma=(s_t,a^{\text{human}}_t,\dots,s_{t+T-1},a^{\text{human}}_{t+T-1})\), 
           aiming at guiding the learning agent out of the region that is dangerous or no longer needs exploration. <br>
        &ensp; 3. Providing a preference signal \(p_t=P_\psi[\sigma\succ \sigma']\in\{0, 0.5, 1\}\), 
           indicating whether current segment  \(\sigma\) is better than the previous segment \(\sigma'\). 

           <!-- As human participants are familiar with recent trajectories of themselves, this way of preference pair construction 
           saves the burden for humans of reviewing previously sampled trajectories.
           Bad human samples can happen due to imperfect human behaviors or untimely intervention decision of the gating agent. 
           By assigning low human preference on these samples, the gating agent can learn from human demonstrations at all performance levels 
           and mistakes in the intervention decision made by itself. -->
   </div>
   <div class="body">

   </div>
</div> 
<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Results</div>
  <div class="body">

    <div class="teaser" style="width: 85%; margin-left: 50pt">
      <img src="assets/walker2d_ablation-1.png">
      <div class="text">
          <br>
          Fig. 3: Learning curves of methods in ablation study. We consider the Walker2d environment. 
          The lines are average return across four different trials and the shadow areas denote the standard deviation.
      </div>
    </div>


  </div>
</div>
<!-- === Result Section Ends === -->


</body>
</html>
