<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Policy Optimization under Imperfect Human Interactions with Agent-Gated Shared Autonomy</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <!-- <div class="logo">
      <a href="https://genforce.github.io/" target="_blank"><img src="./assets/genforce.png"></a>
    </div> -->
    <div class="title", style="padding-top: 25pt;">  <!-- Set padding as 10 if title is with two lines. -->
     Policy Optimization under Imperfect Human <br> Interactions with Agent-Gated Shared Autonomy
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <p style="text-align:center">Submission to ICLR 2025</p>
    <a href="#" target="_blank">Anonymous Authors</a>&nbsp;&nbsp;

</div>

  <!-- <div class="teaser">
    <img src="https://via.placeholder.com/300x100">
  </div> -->
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Overview</div>
  <div class="teaser" style="width: 85%; margin-left: 70pt">
    <img src="assets/overview.png">
    <div class="text">
        <br>
        Fig. 1: Overview of the proposed method.
    </div>
</div>
  <div class="body">
    As shown in Fig. 1, we include a teacher policy \(\pi_t\) in the training loop of RL. 
    During the training of the student policy, both \(\pi_s\) and \(\pi_t\) receive current state \(s\) from the environment. 
    They propose actions \(a_s\) and \(a_t\), and then a value-based intervention function \(\mathcal{T}(s)\) determines which 
    action should be taken and applied to the environment. The student policy is then updated with data collected 
    through both policies.   
  </div>
</div>
<!-- === Overview Section Ends === -->

<div class="section">
  <div class="title">Method</div>
  <div class="body">

    </div>
  <div class="teaser" style="width: 85%; margin-left: 70pt">
    <img src="assets/different_takeover.png">
    <div class="text">
        <br>
        Fig. 2:     In an autonomous driving scenario, the ego vehicle is the blue one on the left, following the gray vehicle on the right. The upper trajectory is proposed by the student to overtake and the lower trajectory is proposed by the teacher to keep following. 
    </div>
  </div>
  <div class="body">

   </div>
   <div class="body">

   </div>
</div> 
<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Results</div>
  <div class="body">
    The training result  with three different levels of teacher policy can be seen in Fig. 3. 
    The first row shows that the performance of TS2C is not limited by the imperfect teacher policies.
     It converges within 200k steps, independent of different performances of the teacher. 
     EGPO and Importance Advicing is clearly bounded by teacher-medium and teacher-low, 
     performing much worse than TS2C with imperfect teachers. The second row of Fig. 3 shows TS2C has lower training cost than both algorithms. 
    <div class="teaser" style="width: 85%; margin-left: 50pt">
      <img src="assets/q1.png">
      <div class="text">
          <br>
          Fig. 3: Comparison between our method TS2C and other algorithms with teacher policies providing 
          online demonstrations. "Importance" refers to the Importance Advising algorithm. 
          For each column, the involved teacher policy has high, medium, and low performance respectively. 
      </div>
    </div>

    The performances of TS2C in different MuJoCo environments are presented in Fig. 4. The figures show that 
    TS2C is generalizable to different environments. It can outperform SAC in all three MuJoCo environments taken into
consideration. On the other hand, though the EGPO algorithm has the best performance in the
Pendulum environment, it struggles in the other two environments, namely Hopper and Walker
    <div class="teaser" style="width: 85%; margin-left: 50pt">
      <img src="assets/mujoco_thin.png">
      <div class="text">
          <br>
          Fig. 4: Performance comparison between our method TS2C and baseline algorithms on three
          environments from MuJoCo.
      </div>
    </div>
  </div>
</div>
<!-- === Result Section Ends === -->


</body>
</html>
